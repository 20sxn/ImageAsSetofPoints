{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zZTEcBCSGql"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH=\"datasets\""
      ],
      "metadata": {
        "id": "xO0N7azqSKm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imgs_to_SoP(imgs):\n",
        "    shape = imgs.shape\n",
        "    height = shape[-2]\n",
        "    width = shape[-1]\n",
        "    batch = shape[0]\n",
        "    \n",
        "    idx = torch.nonzero(torch.ones(height,width)).float()\n",
        "    idx[:,0] = idx[:,0]/height - 0.5\n",
        "    idx[:,1] = idx[:,1]/width - 0.5\n",
        "    idx = idx.T\n",
        "    \n",
        "    flat_imgs = imgs.flatten(start_dim=len(shape)-2)\n",
        "    SoP = torch.cat((flat_imgs,idx.repeat(batch,1,1)),dim=1)\n",
        "    \n",
        "    return SoP"
      ],
      "metadata": {
        "id": "3dNBlrhESMTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True\n",
        "batch_size = 64\n",
        "train_dataset = datasets.CIFAR10(PATH, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        # Set of transformations to apply, we have as input a PIL image (Python \n",
        "        #Image Library). Refer to the PyTorch documentation in the torchvision.transforms package\n",
        "        transforms.ToTensor (), # Transform the PIL image to a torch.Tensor\n",
        "        transforms.Normalize((0.491, 0.482, 0.447), (0.202, 0.199, 0.201)),\n",
        "        #transforms.Lambda(imgs_to_SoP) #on l'utilise dans le reseau\n",
        "        \n",
        "        #transforms.RandomCrop(28),\n",
        "        #transforms.Pad(2)\n",
        "        #transforms.RandomHorizontalFlip(p=0.5)\n",
        "    ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57F29DW6SNR8",
        "outputId": "634dab23-9d32-4667-c935-3ccbbd115aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = next(iter(train_loader))\n",
        "print(X.shape)\n",
        "print(imgs_to_SoP(X).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fIYWdFrTKeT",
        "outputId": "2b1257df-f717-4585-cb78-0e7ef0fac966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 5, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PointReducer(nn.Module):\n",
        "    def __init__(self,in_chan,out_chan,kernel_size=2,stride = 2):\n",
        "        super().__init__()\n",
        "        self.conv2d = nn.Conv2d(in_chan,out_chan,kernel_size,stride)\n",
        "    def forward(self,input):\n",
        "        return self.conv2d(input)"
      ],
      "metadata": {
        "id": "0oxyFOcMYCBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,res):\n",
        "        super().__init__()\n",
        "        self.res = res #sequence of +(Point reducer, context cluster block)\n",
        "\n",
        "    def imgs_to_SoP(self,imgs):\n",
        "        \"\"\"\n",
        "        Transform a batch of images to a bacth of sets of points\n",
        "\n",
        "        imgs : torch.Tensor([batch,chan,height,width])\n",
        "        return : torch.Tensor([batch,chan+2,height*width])\n",
        "        \"\"\"\n",
        "        shape = imgs.shape\n",
        "        height = shape[-2]\n",
        "        width = shape[-1]\n",
        "        batch = shape[0]\n",
        "        \n",
        "        idx = torch.nonzero(torch.ones(height,width)).float()\n",
        "        idx[:,0] = idx[:,0]/height - 0.5\n",
        "        idx[:,1] = idx[:,1]/width - 0.5\n",
        "        idx = idx.T\n",
        "        \n",
        "        flat_imgs = imgs.flatten(start_dim=len(shape)-2)\n",
        "        SoP = torch.cat((flat_imgs,idx.repeat(batch,1,1)),dim=1)\n",
        "        \n",
        "        return SoP\n",
        "\n",
        "    def forward(self,input):\n",
        "        SoP = self.imgs_to_SoP(input)\n",
        "        pred = self.res\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "wSDd__2hSReU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}